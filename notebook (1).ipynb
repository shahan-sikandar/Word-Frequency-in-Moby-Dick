{"cells":[{"source":"![mobydick](mobydick.jpg)","metadata":{},"id":"b1309988-b429-4fb0-8c4c-193582dbec93","cell_type":"markdown"},{"source":"In this workspace, you'll scrape the novel Moby Dick from the website [Project Gutenberg](https://www.gutenberg.org/) (which contains a large corpus of books) using the Python `requests` package. You'll extract words from this web data using `BeautifulSoup` before analyzing the distribution of words using the Natural Language ToolKit (`nltk`) and `Counter`.\n\nThe Data Science pipeline you'll build in this workspace can be used to visualize the word frequency distributions of any novel you can find on Project Gutenberg.","metadata":{},"id":"611e416c-70e7-478a-a3c8-e54f3fdb4a7f","cell_type":"markdown"},{"source":"# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nnltk.download('stopwords')\n\n# Start coding here... ","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"executionTime":53,"lastSuccessfullyExecutedCode":"# Import and download packages\nimport requests\nfrom bs4 import BeautifulSoup\nimport nltk\nfrom collections import Counter\nnltk.download('stopwords')\n\n# Start coding here... ","executionCancelledAt":null,"lastExecutedAt":1701463165310,"lastScheduledRunId":null,"outputsMetadata":{"0":{"height":56,"type":"stream"}}},"id":"15b5f52f-fd9b-4f0e-9fcc-f7733022c7c0","cell_type":"code","execution_count":44,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package stopwords to /home/repl/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"},{"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{},"execution_count":44}]},{"source":"#  1. Request and encode the text","metadata":{},"cell_type":"markdown","id":"b1f6cb59-d356-4f1b-aaa5-4854b9a943bc"},{"source":"# Get the Moby Dick HTML  \nurl = \"https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm\"\n\n# Getting Moby Dick HTML\nr = requests.get(url)\n\n# Set the correct text encoding of the HTML page\nr.encoding = 'utf-8'","metadata":{"executionCancelledAt":null,"executionTime":117,"lastExecutedAt":1701463165427,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Get the Moby Dick HTML  \nurl = \"https://s3.amazonaws.com/assets.datacamp.com/production/project_147/datasets/2701-h.htm\"\n\n# Getting Moby Dick HTML\nr = requests.get(url)\n\n# Set the correct text encoding of the HTML page\nr.encoding = 'utf-8'","outputsMetadata":{"0":{"height":580,"type":"stream"}}},"cell_type":"code","id":"b4d20fa1-e37a-496e-b43d-f62837dc72a9","execution_count":45,"outputs":[]},{"source":"#  2. Extract the text","metadata":{},"cell_type":"markdown","id":"8bdf2983-8c5c-4e2f-ae85-4bc80fc2b778"},{"source":"# Extract the HTML from the request object\nhtml = r.text\n\n# Print the first 2000 characters in html\nprint(html[:2000])","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1701463165477,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Extract the HTML from the request object\nhtml = r.text\n\n# Print the first 2000 characters in html\nprint(html[:2000])","outputsMetadata":{"0":{"height":375,"type":"stream"}}},"cell_type":"code","id":"30a1342b-f870-41fc-af13-0d75fdd445d0","execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n\n<!DOCTYPE html\n   PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\n   \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\" >\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\" lang=\"en\">\n  <head>\n    <title>\n      Moby Dick; Or the Whale, by Herman Melville\n    </title>\n    <style type=\"text/css\" xml:space=\"preserve\">\n\n    body { background:#faebd0; color:black; margin-left:15%; margin-right:15%; text-align:justify }\n    P { text-indent: 1em; margin-top: .25em; margin-bottom: .25em; }\n    H1,H2,H3,H4,H5,H6 { text-align: center; margin-left: 15%; margin-right: 15%; }\n    hr  { width: 50%; text-align: center;}\n    .foot { margin-left: 20%; margin-right: 20%; text-align: justify; text-indent: -3em; font-size: 90%; }\n    blockquote {font-size: 100%; margin-left: 0%; margin-right: 0%;}\n    .mynote    {background-color: #DDE; color: #000; padding: .5em; margin-left: 10%; margin-right: 10%; font-family: sans-serif; font-size: 95%;}\n    .toc       { margin-left: 10%; margin-bottom: .75em;}\n    .toc2      { margin-left: 20%;}\n    div.fig    { display:block; margin:0 auto; text-align:center; }\n    div.middle { margin-left: 20%; margin-right: 20%; text-align: justify; }\n    .figleft   {float: left; margin-left: 0%; margin-right: 1%;}\n    .figright  {float: right; margin-right: 0%; margin-left: 1%;}\n    .pagenum   {display:inline; font-size: 70%; font-style:normal;\n               margin: 0; padding: 0; position: absolute; right: 1%;\n               text-align: right;}\n    pre        { font-family: times new roman; font-size: 100%; margin-left: 10%;}\n\n    table      {margin-left: 10%;}\n\na:link {color:blue;\n\t\ttext-decoration:none}\nlink {color:blue;\n\t\ttext-decoration:none}\na:visited {color:blue;\n\t\ttext-decoration:none}\na:hover {color:red}\n\n</style>\n  </head>\n  <body>\n<pre xml:space=\"preserve\">\n\nThe Project Gutenberg EBook of Moby Dick; or The Whale, by Herman Melville\n\nThis eBook is for the use of anyone anywh\n"}]},{"source":"# 3. Create a BeautifulSoup object and get the text","metadata":{},"cell_type":"markdown","id":"ea6fe5bc-ea1a-492a-b1f7-758ebd2bbcee"},{"source":"# Create a BeautifulSoup object from HTML\nhtml_soup = BeautifulSoup(html, \"html.parser\")\n\n# Get the text out of the soup\nmoby_text = html_soup.get_text()","metadata":{"executionCancelledAt":null,"executionTime":240,"lastExecutedAt":1701463165718,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create a BeautifulSoup object from HTML\nhtml_soup = BeautifulSoup(html, \"html.parser\")\n\n# Get the text out of the soup\nmoby_text = html_soup.get_text()","outputsMetadata":{"0":{"height":580,"type":"stream"}}},"cell_type":"code","id":"83692f11-e821-413e-990d-27a6d4f14ce9","execution_count":47,"outputs":[]},{"source":"# 4. Tokenize the text","metadata":{},"cell_type":"markdown","id":"e4eeb953-7c26-4222-8611-fd25d3106913"},{"source":"# Create a tokenizer\ntokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n\n# Tokenize the text\ntokens = tokenizer.tokenize(moby_text)","metadata":{"executionCancelledAt":null,"executionTime":90,"lastExecutedAt":1701463165809,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create a tokenizer\ntokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n\n# Tokenize the text\ntokens = tokenizer.tokenize(moby_text)","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"cell_type":"code","id":"e62d406b-e3a3-4ee8-8513-378c5e667084","execution_count":48,"outputs":[]},{"source":"# 5. Convert words to lowercase","metadata":{},"cell_type":"markdown","id":"71ac4e0a-c686-49dc-b598-3c817a2366a4"},{"source":"# Create a list called words containing all tokens transformed to lowercase\nwords = [token.lower() for token in tokens]\n\n# Print the first 8 words\nprint(words[:8])","metadata":{"executionCancelledAt":null,"executionTime":51,"lastExecutedAt":1701463165861,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create a list called words containing all tokens transformed to lowercase\nwords = [token.lower() for token in tokens]\n\n# Print the first 8 words\nprint(words[:8])","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"cell_type":"code","id":"9fef0579-0b63-4a73-af26-f683f4f8359e","execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":"['moby', 'dick', 'or', 'the', 'whale', 'by', 'herman', 'melville']\n"}]},{"source":"# 6. Load in stop words","metadata":{},"cell_type":"markdown","id":"7f380f8f-ff6e-4870-b4da-bf5c449e9735"},{"source":"# Get the English stopwords from nltk\nstop_words = nltk.corpus.stopwords.words('english')\n\n# Print the first 8 words\nprint(stop_words[:8])","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1701463165909,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Get the English stopwords from nltk\nstop_words = nltk.corpus.stopwords.words('english')\n\n# Print the first 8 words\nprint(stop_words[:8])","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"cell_type":"code","id":"c60c7a2f-94bd-49ed-8fab-596071aeb9bd","execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":"['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves']\n"}]},{"source":"# 7. Remove stop words from the text","metadata":{},"cell_type":"markdown","id":"1b9d3f6b-2ff8-4870-a3b4-babc97939d00"},{"source":"# Create a list words_ns containing all words that are in words but not in stop_words\nwords_no_stop = [word for word in words if word not in stop_words]\n\n# Print the first five words_no_stop to check that stop words are gone\nprint(words_no_stop[:5])","metadata":{"executionCancelledAt":null,"executionTime":380,"lastExecutedAt":1701463166289,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Create a list words_ns containing all words that are in words but not in stop_words\nwords_no_stop = [word for word in words if word not in stop_words]\n\n# Print the first five words_no_stop to check that stop words are gone\nprint(words_no_stop[:5])","outputsMetadata":{"0":{"height":37,"type":"stream"}}},"cell_type":"code","id":"5c82dcfc-fec4-48cb-8f00-f7b622ce1e4f","execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":"['moby', 'dick', 'whale', 'herman', 'melville']\n"}]},{"source":"# 8. Count the frequency of words","metadata":{},"cell_type":"markdown","id":"d9cb84bf-8cfa-4742-9138-09d07c39dbc5"},{"source":"# Initialize a Counter object from our processed list of words\ncount = Counter(words_no_stop)\n\n# Store ten most common words and their counts as top_ten\ntop_ten = count.most_common(10)\n\n# Print the top 10 words and their count\nprint(top_ten)","metadata":{"executionCancelledAt":null,"executionTime":64,"lastExecutedAt":1701463166353,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Initialize a Counter object from our processed list of words\ncount = Counter(words_no_stop)\n\n# Store ten most common words and their counts as top_ten\ntop_ten = count.most_common(10)\n\n# Print the top 10 words and their count\nprint(top_ten)","outputsMetadata":{"0":{"height":56,"type":"stream"}}},"cell_type":"code","id":"fd7803b7-d893-4f8b-b8b8-71d1557a87bf","execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":"[('whale', 1246), ('one', 925), ('like', 647), ('upon', 568), ('man', 527), ('ship', 519), ('ahab', 517), ('ye', 473), ('sea', 455), ('old', 452)]\n"}]}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":5}